{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.002, loss: 0.500 (data_loss: 0.500, reg_loss: 0.000), lr: 0.005\n",
      "epoch: 100, acc: 0.007, loss: 0.084 (data_loss: 0.084, reg_loss: 0.000), lr: 0.004549590536851684\n",
      "epoch: 200, acc: 0.033, loss: 0.034 (data_loss: 0.034, reg_loss: 0.000), lr: 0.004170141784820684\n",
      "epoch: 300, acc: 0.020, loss: 0.003 (data_loss: 0.003, reg_loss: 0.000), lr: 0.003849114703618168\n",
      "epoch: 400, acc: 0.618, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0035739814152966403\n",
      "epoch: 500, acc: 0.605, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00333555703802535\n",
      "epoch: 600, acc: 0.732, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0031269543464665416\n",
      "epoch: 700, acc: 0.763, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.002942907592701589\n",
      "epoch: 800, acc: 0.778, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0027793218454697055\n",
      "epoch: 900, acc: 0.789, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0026329647182727752\n",
      "epoch: 1000, acc: 0.135, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.002501250625312656\n",
      "epoch: 1100, acc: 0.825, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0023820867079561697\n",
      "epoch: 1200, acc: 0.837, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.002273760800363802\n",
      "epoch: 1300, acc: 0.453, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.002174858634188778\n",
      "epoch: 1400, acc: 0.853, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0020842017507294707\n",
      "epoch: 1500, acc: 0.857, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0020008003201280513\n",
      "epoch: 1600, acc: 0.861, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001923816852635629\n",
      "epoch: 1700, acc: 0.869, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001852537977028529\n",
      "epoch: 1800, acc: 0.874, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0017863522686673815\n",
      "epoch: 1900, acc: 0.882, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0017247326664367024\n",
      "epoch: 2000, acc: 0.885, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0016672224074691564\n",
      "epoch: 2100, acc: 0.887, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0016134236850596968\n",
      "epoch: 2200, acc: 0.891, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0015629884338855893\n",
      "epoch: 2300, acc: 0.896, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0015156107911488332\n",
      "epoch: 2400, acc: 0.899, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0014710208884966167\n",
      "epoch: 2500, acc: 0.898, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0014289797084881396\n",
      "epoch: 2600, acc: 0.906, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001389274798555154\n",
      "epoch: 2700, acc: 0.908, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0013517166801838335\n",
      "epoch: 2800, acc: 0.919, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0013161358252171624\n",
      "epoch: 2900, acc: 0.920, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0012823800974608873\n",
      "epoch: 3000, acc: 0.156, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0012503125781445363\n",
      "epoch: 3100, acc: 0.929, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0012198097096852891\n",
      "epoch: 3200, acc: 0.927, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0011907597046915933\n",
      "epoch: 3300, acc: 0.190, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0011630611770179114\n",
      "epoch: 3400, acc: 0.927, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0011366219595362584\n",
      "epoch: 3500, acc: 0.928, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0011113580795732384\n",
      "epoch: 3600, acc: 0.915, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0010871928680147858\n",
      "epoch: 3700, acc: 0.935, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0010640561821664183\n",
      "epoch: 3800, acc: 0.933, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0010418837257762034\n",
      "epoch: 3900, acc: 0.242, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0010206164523372118\n",
      "epoch: 4000, acc: 0.935, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0010002000400080014\n",
      "epoch: 4100, acc: 0.934, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0009805844283192783\n",
      "epoch: 4200, acc: 0.748, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0009617234083477593\n",
      "epoch: 4300, acc: 0.935, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0009435742592942063\n",
      "epoch: 4400, acc: 0.936, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0009260974254491572\n",
      "epoch: 4500, acc: 0.930, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0009092562284051646\n",
      "epoch: 4600, acc: 0.937, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.000893016610108948\n",
      "epoch: 4700, acc: 0.463, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0008773469029654326\n",
      "epoch: 4800, acc: 0.941, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.000862217623728229\n",
      "epoch: 4900, acc: 0.943, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0008476012883539582\n",
      "epoch: 5000, acc: 0.877, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0008334722453742291\n",
      "epoch: 5100, acc: 0.944, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0008198065256599442\n",
      "epoch: 5200, acc: 0.944, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0008065817067268914\n",
      "epoch: 5300, acc: 0.463, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0007937767899666614\n",
      "epoch: 5400, acc: 0.943, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0007813720893889669\n",
      "epoch: 5500, acc: 0.945, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0007693491306354824\n",
      "epoch: 5600, acc: 0.155, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0007576905591756327\n",
      "epoch: 5700, acc: 0.948, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0007463800567248844\n",
      "epoch: 5800, acc: 0.949, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0007354022650389764\n",
      "epoch: 5900, acc: 0.950, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0007247427163357008\n",
      "epoch: 6000, acc: 0.950, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.000714387769681383\n",
      "epoch: 6100, acc: 0.950, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0007043245527539089\n",
      "epoch: 6200, acc: 0.901, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006945409084595084\n",
      "epoch: 6300, acc: 0.952, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006850253459377996\n",
      "epoch: 6400, acc: 0.951, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006757669955399379\n",
      "epoch: 6500, acc: 0.953, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006667555674089878\n",
      "epoch: 6600, acc: 0.957, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006579813133307014\n",
      "epoch: 6700, acc: 0.953, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006494349915573451\n",
      "epoch: 6800, acc: 0.955, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006411078343377356\n",
      "epoch: 6900, acc: 0.958, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00063299151791366\n",
      "epoch: 7000, acc: 0.957, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006250781347668457\n",
      "epoch: 7100, acc: 0.958, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006173601679219657\n",
      "epoch: 7200, acc: 0.744, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006098304671301379\n",
      "epoch: 7300, acc: 0.964, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006024822267743102\n",
      "epoch: 7400, acc: 0.957, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005953089653530181\n",
      "epoch: 7500, acc: 0.954, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.000588304506412519\n",
      "epoch: 7600, acc: 0.964, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005814629608093965\n",
      "epoch: 7700, acc: 0.966, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005747787101965744\n",
      "epoch: 7800, acc: 0.960, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005682463916354131\n",
      "epoch: 7900, acc: 0.966, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005618608832453085\n",
      "epoch: 8000, acc: 0.961, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00055561729081009\n",
      "epoch: 8100, acc: 0.867, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005495109352676119\n",
      "epoch: 8200, acc: 0.964, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005435373410153278\n",
      "epoch: 8300, acc: 0.955, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005376922249704269\n",
      "epoch: 8400, acc: 0.968, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005319714863283328\n",
      "epoch: 8500, acc: 0.966, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005263711969681019\n",
      "epoch: 8600, acc: 0.966, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005208875924575476\n",
      "epoch: 8700, acc: 0.967, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005155170636148056\n",
      "epoch: 8800, acc: 0.972, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005102561485865905\n",
      "epoch: 8900, acc: 0.969, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005051015254066068\n",
      "epoch: 9000, acc: 0.967, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005000500050005\n",
      "epoch: 9100, acc: 0.968, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0004950985246063966\n",
      "epoch: 9200, acc: 0.959, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0004902441415825081\n",
      "epoch: 9300, acc: 0.971, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0004854840275754928\n",
      "epoch: 9400, acc: 0.968, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0004808154630252909\n",
      "epoch: 9500, acc: 0.964, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00047623583198399844\n",
      "epoch: 9600, acc: 0.969, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00047174261722804036\n",
      "epoch: 9700, acc: 0.969, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00046733339564445275\n",
      "epoch: 9800, acc: 0.972, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00046300583387350687\n",
      "epoch: 9900, acc: 0.964, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00045875768419121016\n",
      "epoch: 10000, acc: 0.970, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00045458678061641964\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Neral Networks from Scratch in Python by page 458\n",
    "\n",
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import sine_data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "    \n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons, \n",
    "                 weight_regularizer_l1=0, weight_regularizer_l2=0, \n",
    "                 bias_regularizer_l1=0, bias_regularizer_l2=0): \n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01*np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        # Set regularization strength\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "        \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases \n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues) \n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        \n",
    "        # Gradients on regularization\n",
    "        # L1 on weights\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "        \n",
    "        # L2 on weights\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * \\\n",
    "            self.weights\n",
    "\n",
    "        # L1 on biases\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "\n",
    "        # L2 on biases\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * \\\n",
    "            self.biases\n",
    "\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "        \n",
    "# Dropout\n",
    "class Layer_Dropout:\n",
    "\n",
    "    # Init\n",
    "    def __init__(self, rate):\n",
    "        # Store rate, we invert it as for example for dropout \n",
    "        # of 0.1 we need success rate of 0.9\n",
    "        self.rate = 1 - rate\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Save input values\n",
    "        self.inputs = inputs\n",
    "        # Generate and save scaled mask\n",
    "        self.binary_mask = np.random.binomial(1, self.rate,\n",
    "                                              size=inputs.shape)/self.rate \n",
    "        # Apply mask to output values\n",
    "        self.output = inputs*self.binary_mask\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradient on values\n",
    "        self.dinputs = dvalues*self.binary_mask\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs \n",
    "        self.output = np.maximum(0, inputs)\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable, \n",
    "        # let's make a copy of values first \n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0        \n",
    "        \n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs): \n",
    "        # Remember input values \n",
    "        self.inputs = inputs\n",
    "        \n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values/np.sum(exp_values, axis=1,\n",
    "                                          keepdims=True)\n",
    "        self.output = probabilities        \n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        \n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in \\\n",
    "                enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            \n",
    "            # Calculate Jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - \\\n",
    "                              np.dot(single_output, single_output.T)\n",
    "    \n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
    "                                         single_dvalues)\n",
    "\n",
    "# Sigmoid activation\n",
    "class Activation_Sigmoid:\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Save input and calculate/save output # of the sigmoid function\n",
    "        self.inputs = inputs\n",
    "        self.output = 1/(1 + np.exp(-inputs))\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Derivative - calculates from output of the sigmoid function \n",
    "        self.dinputs = dvalues*(1 - self.output)*self.output\n",
    "\n",
    "\n",
    "# Linear activation\n",
    "class Activation_Linear:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Just remember values\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # derivative is 1, 1*dvalues = dvalues - chain rule\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "\n",
    "# SGD optimizer\n",
    "class Optimizer_SGD:\n",
    "    \n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum    \n",
    "    \n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self): \n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay*self.iterations))\n",
    "        \n",
    "    # Update parameters\n",
    "    def update_params(self, layer): \n",
    "        \n",
    "        # If we use momentum\n",
    "        if self.momentum:\n",
    "            \n",
    "            # If layer does not contain momentum arrays, create them\n",
    "            # filled with zeros\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                # If there is no momentum array for weights\n",
    "                # The array doesn't exist for biases yet either.\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "        \n",
    "            # Build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            weight_updates = \\\n",
    "                self.momentum*layer.weight_momentums - \\\n",
    "                self.current_learning_rate*layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "        \n",
    "            # Build bias updates\n",
    "            bias_updates = \\\n",
    "                self.momentum*layer.bias_momentums - \\\n",
    "                self.current_learning_rate*layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "        \n",
    "        # Vanilla SGD updates (as before momentum update)\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * \\\n",
    "                             layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * \\\n",
    "                           layer.dbiases\n",
    "            \n",
    "        # Update weights and biases using either\n",
    "        # vanilla or momentum updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self): \n",
    "        self.iterations += 1\n",
    "      \n",
    "    \n",
    "# Adagrad optimizer\n",
    "class Optimizer_Adagrad:\n",
    "    \n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7): \n",
    "        self.learning_rate = learning_rate \n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self): \n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay*self.iterations))\n",
    "    \n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "        \n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                         layer.dweights / \\\n",
    "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                        layer.dbiases / \\\n",
    "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "    \n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self): \n",
    "        self.iterations += 1\n",
    "    \n",
    "# RMSprop optimizer\n",
    "class Optimizer_RMSprop:\n",
    "    \n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "    \n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self): \n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "    \n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + \\\n",
    "            (1 - self.rho) * layer.dweights**2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + \\\n",
    "            (1 - self.rho) * layer.dbiases**2    \n",
    "    \n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                         layer.dweights / \\\n",
    "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                        layer.dbiases / \\\n",
    "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self): \n",
    "        self.iterations += 1\n",
    "    \n",
    "# Adam optimizer\n",
    "class Optimizer_Adam:\n",
    "      \n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "    \n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self): \n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "      \n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "          \n",
    "        # Update momentum  with current gradients\n",
    "        layer.weight_momentums = self.beta_1 * \\\n",
    "                                 layer.weight_momentums + \\\n",
    "                                 (1-self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * \\\n",
    "                               layer.bias_momentums + \\\n",
    "                               (1-self.beta_1) * layer.dbiases\n",
    "          \n",
    "        # Get corrected momentum\n",
    "        # self.iteration is 0 at first pass\n",
    "        # and we need to start with 1 here\n",
    "        weight_momentums_corrected = layer.weight_momentums / \\\n",
    "                                     (1-self.beta_1**(self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / \\\n",
    "                                   (1-self.beta_1**(self.iterations + 1))\n",
    "        \n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.beta_2*layer.weight_cache + \\\n",
    "                             (1-self.beta_2)*layer.dweights**2\n",
    "        \n",
    "        layer.bias_cache = self.beta_2*layer.bias_cache + \\\n",
    "                           (1-self.beta_2)*layer.dbiases**2\n",
    "        \n",
    "        # Get corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / \\\n",
    "                                 (1-self.beta_2**(self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / \\\n",
    "                               (1-self.beta_2**(self.iterations + 1))\n",
    "        \n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                         weight_momentums_corrected / \\\n",
    "                         (np.sqrt(weight_cache_corrected) +\n",
    "                             self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                         bias_momentums_corrected / \\\n",
    "                         (np.sqrt(bias_cache_corrected) +\n",
    "                             self.epsilon)\n",
    "    \n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self): \n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Regularization loss calculation\n",
    "    def regularization_loss(self, layer): \n",
    "\n",
    "        # 0 by default\n",
    "        regularization_loss = 0\n",
    "        \n",
    "        # L1 regularization - weights\n",
    "        # calculate only when factor greater than 0\n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * \\\n",
    "                                   np.sum(np.abs(layer.weights))\n",
    "        \n",
    "        # L2 regularization - weights\n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * \\\n",
    "                                   np.sum(layer.weights * \\\n",
    "                                          layer.weights)\n",
    "            \n",
    "        # L1 regularization - biases\n",
    "        # calculate only when factor greater than 0\n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * \\\n",
    "                                   np.sum(np.abs(layer.biases))\n",
    "\n",
    "        # L2 regularization - biases\n",
    "        if layer.bias_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l2 * \\\n",
    "                                   np.sum(layer.biases * \\\n",
    "                                          layer.biases)\n",
    "        return regularization_loss\n",
    "\n",
    "\n",
    "    # Calculates the data and regularization losses \n",
    "    # given model output and ground truth values \n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        \n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss): \n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true): \n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true \n",
    "            ]\n",
    "            \n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1 )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "        \n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "        \n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true/dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs/samples\n",
    "\n",
    "\n",
    "\n",
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    \n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax() \n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function \n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true): \n",
    "        \n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "            \n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "# Binary cross-entropy loss\n",
    "class Loss_BinaryCrossentropy(Loss): \n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # Calculate sample-wise loss\n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) +\n",
    "                          (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "        sample_losses = np.mean(sample_losses, axis=-1) \n",
    "        \n",
    "        # Return losses\n",
    "        return sample_losses        \n",
    "        \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # Calculate gradient\n",
    "        self.dinputs = -(y_true / clipped_dvalues -\n",
    "                         (1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
    "        \n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs/samples     \n",
    "\n",
    "# Mean Squared Error loss\n",
    "class Loss_MeanSquaredError(Loss): # L2 loss\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Calculate loss\n",
    "        sample_losses = np.mean((y_true-y_pred)**2, axis=-1)\n",
    "\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        # Gradient on values\n",
    "        self.dinputs = -2 * (y_true - dvalues)/outputs\n",
    "        \n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs/samples\n",
    "\n",
    "\n",
    "# Mean Absolute Error loss\n",
    "class Loss_MeanAbsoluteError(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Calculate loss\n",
    "        sample_losses = np.mean(np.abs(y_true - y_pred), axis=-1)\n",
    "\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = np.sign(y_true-dvalues)/outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs/samples\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "X, y = sine_data()\n",
    "\n",
    "# Create Dense layer with 1 input feature and 64 output values\n",
    "dense1 = Layer_Dense(1, 64)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 64 output values\n",
    "dense2 = Layer_Dense(64, 64)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation2 = Activation_ReLU()\n",
    "\n",
    "# Create third Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 1 output value\n",
    "dense3 = Layer_Dense(64, 1)\n",
    "\n",
    "# Create Linear activation:\n",
    "activation3 = Activation_Linear()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_MeanSquaredError()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_Adam(learning_rate=0.005, decay=1e-3)\n",
    "\n",
    "# Accuracy precision for accuracy calculation\n",
    "# There are no really accuracy factor for regression problem,\n",
    "# but we can simulate/approximate it. We'll calculate it by checking\n",
    "# how many values have a difference to their ground truth equivalent\n",
    "# less than given precision\n",
    "# We'll calculate this precision as a fraction of standard deviation\n",
    "# of all the ground truth values\n",
    "accuracy_precision = np.std(y) / 250\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function\n",
    "    # of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of second dense layer here\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    # Perform a forward pass through third Dense layer\n",
    "    # takes outputs of activation function of second layer as inputs\n",
    "    dense3.forward(activation2.output)\n",
    "\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of third dense layer here\n",
    "    activation3.forward(dense3.output)\n",
    "\n",
    "    # Calculate the data loss\n",
    "    data_loss = loss_function.calculate(activation3.output, y)\n",
    "\n",
    "    # Calculate regularization penalty\n",
    "    regularization_loss = \\\n",
    "        loss_function.regularization_loss(dense1) + \\\n",
    "        loss_function.regularization_loss(dense2) + \\\n",
    "        loss_function.regularization_loss(dense3)\n",
    "\n",
    "    # Calculate overall loss\n",
    "    loss = data_loss + regularization_loss\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # To calculate it we're taking absolute difference between\n",
    "    # predictions and ground truth values and compare if differences\n",
    "    # are lower than given precision value\n",
    "    predictions = activation3.output\n",
    "    accuracy = np.mean(np.absolute(predictions - y) <\n",
    "                       accuracy_precision)\n",
    "\n",
    "    if not epoch % 100: \n",
    "        print(f'epoch: {epoch}, ' +\n",
    "            f'acc: {accuracy:.3f}, ' +\n",
    "            f'loss: {loss:.3f} (' +\n",
    "            f'data_loss: {data_loss:.3f}, ' + \n",
    "            f'reg_loss: {regularization_loss:.3f}), ' + \n",
    "            f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss_function.backward(activation3.output, y)\n",
    "    activation3.backward(loss_function.dinputs)\n",
    "    dense3.backward(activation3.dinputs)\n",
    "    activation2.backward(dense3.dinputs)\n",
    "    dense2.backward(activation2.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.update_params(dense3)\n",
    "    optimizer.post_update_params()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_test, y_test = sine_data()\n",
    "\n",
    "dense1.forward(X_test)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "dense3.forward(activation2.output)\n",
    "activation3.forward(dense3.output)\n",
    "\n",
    "plt.plot(X_test, y_test)\n",
    "plt.plot(X_test, activation3.output)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
